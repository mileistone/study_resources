##### 2021
- [2105.07576] [Rethinking "Batch" in BatchNorm](https://arxiv.org/abs/2105.07576)
- [2105.01601] [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601)

##### 2020
- [CVPR2020] [Learning in the Frequency Domain](https://arxiv.org/abs/2002.12416)
  - [DCT](https://www.math.cuhk.edu.hk/~lmlui/dct.pdf)
  - [NIPS2018] [Faster Neural Networks Straight from JPEG](https://papers.nips.cc/paper/7649-faster-neural-networks-straight-from-jpeg.pdf) 
  - [IMVIP2017] [On using CNN with DCT based Image Data](https://www.scss.tcd.ie/Rozenn.Dahyot/pdf/IMVIP2017_MatejUlicny.pdf)
- [2004.08955] [ResNeSt: Split-Attention Networks](https://arxiv.org/abs/2004.08955)

##### 2019
- [1911.09737] [Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks](https://arxiv.org/abs/1911.09737)
- [1908.01259] [Attentive Normalization](https://arxiv.org/abs/1908.01259)
- [1906.02629] [When Does Label Smoothing Help?](https://arxiv.org/abs/1906.02629)
- [1905.11001] [On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks](https://arxiv.org/abs/1905.11001)
- [1904.11486] [Making Convolutional Networks Shift-Invariant Again](https://arxiv.org/abs/1904.11486)
- [1904.05049] [Drop an octave: Reducing spatial redundancy in convolutional neural networks with octave convolution](https://arxiv.org/pdf/1904.05049)
  - Octave conv 
  - should be compared with [Multigrid Neural Architectures](https://arxiv.org/abs/1611.07661)
- [1904.04971] [Soft Conditional Computation](https://arxiv.org/abs/1904.04971)
- [CVPR2019] [Attention Augmented Convolutional Networks](https://arxiv.org/abs/1904.09925)
- [NIPS2019] [CondConv: Conditionally Parameterized Convolutions for Efficient Inference](https://arxiv.org/abs/1904.04971)

##### 2018
- [CVPR2018] [Non-local neural networks](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.pdf)
- [1811.11718] [Partial Convolution based Padding](https://arxiv.org/abs/1811.11718)
- [1811.11168] [Deformable ConvNets v2: More Deformable, Better Results](https://arxiv.org/abs/1811.11168)
- [1810.12890] [DropBlock: A regularization method for convolutional networks](https://arxiv.org/abs/1810.12890)
- [1807.06521] [CBAM: Convolutional Block Attention Module](https://arxiv.org/abs/1807.06521)
- [1805.12177] [Why do deep convolutional networks generalize so poorly to small image transformations?](https://arxiv.org/abs/1805.12177)
- [1805.11604] [How Does Batch Normalization Help Optimization?](https://arxiv.org/abs/1805.11604)

##### 2017
- [CVPR2017] [Multigrid Neural Architectures](https://arxiv.org/abs/1611.07661)
- [1709.01507] [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507) # SE
- [1708.02002] [Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002)
- [1703.06211] [Deformable Convolutional Networks](https://arxiv.org/pdf/1703.06211.pdf) | [slides](http://presentations.cocodataset.org/COCO17-Detect-MSRA.pdf) | [tutorial](https://towardsdatascience.com/review-dcn-deformable-convolutional-networks-2nd-runner-up-in-2017-coco-detection-object-14e488efce44)
- [1703.02719] [Large Kernel Matters ——
Improve Semantic Segmentation by Global Convolutional Network](https://arxiv.org/abs/1703.02719) #GCN
- [1702.03275] [Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models](https://arxiv.org/abs/1702.03275)

##### 2016
- [1603.05201] [Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units](https://arxiv.org/abs/1603.05201) # CReLU
- [ECCV2016] [Deep Networks with Stochastic Depth](https://arxiv.org/abs/1603.09382)

##### 2015
- [1502.03167] [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167)

##### 2014
- [1412.6806] [Striving for Simplicity: The All Convolutional Net](https://arxiv.org/abs/1412.6806)

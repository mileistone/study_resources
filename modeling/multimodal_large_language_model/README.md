### Paper
- [Awesome-Multimodal-Large-Language-Models](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models)
- [2401.06209] [Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs](https://arxiv.org/abs/2401.06209)
  - use dino and clip visual features
- [2312.07533] [VILA: On Pre-training for Visual Language Models](https://arxiv.org/abs/2312.07533)
- [2306.17107] [LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding](https://arxiv.org/abs/2306.17107)
- [2306.13549] [A Survey on Multimodal Large Language Models](https://arxiv.org/abs/2306.13549)
- [2306.02858] [Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding](https://arxiv.org/abs/2306.02858)
- [2305.06500] [InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning](https://arxiv.org/abs/2305.06500)
- [2305.06355] [VideoChat: Chat-Centric Video Understanding](https://arxiv.org/abs/2305.06355)
- [2304.14178] [mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality](https://arxiv.org/abs/2304.14178)
- [2304.10592] [MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models](https://arxiv.org/abs/2304.10592)
- [2304.08485] [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485) # LLaVA
- [2301.12597] [BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models](https://arxiv.org/abs/2301.12597)

### Slides
- [Multimodal-Deep-Learning-CS224n-Kiela](https://web.stanford.edu/class/cs224n/slides/Multimodal-Deep-Learning-CS224n-Kiela.pdf)
- [CS324 - Large Language Models](https://stanford-cs324.github.io/winter2022/)
- [Multimodality and Large Multimodal Models](https://huyenchip.com/2023/10/10/multimodal.html)
